{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO2ztFi8kJcpcr/Y0KMpl5i",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ijul17/homework/blob/main/Yulia%20Urazova/For_StreamLit_Legprom_last_data_Tfidf_model_with_2_fabrics_4_cols_reduced_with_comments_%D0%B4%D0%BE%D1%80%D0%B0%D0%B1%D0%BE%D1%82%D0%B0%D1%82%D1%8C.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from gensim import corpora,models,similarities\n",
        "from gensim.utils import tokenize\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem.snowball import SnowballStemmer"
      ],
      "metadata": {
        "id": "ExvAUs3NOQNC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aac4abfa-5bec-4035-e105-08ad29b09245"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#import gdown\n",
        "#F_URL = \"https://docs.google.com/spreadsheets/d/1uoj0R7e6pTlX9EK2CvdVOt1FePBIVYyj/edit?usp=sharing&ouid=118340160593820077479&rtpof=true&sd=true\" #файл с фабриками, заказами\n",
        "\n",
        "#file_id = F_URL.split('/')[-2]\n",
        "#id=\"https://drive.google.com/uc?id=\" + file_id\n"
      ],
      "metadata": {
        "id": "qoUF00luo69e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#начальный словарь, полученный на вход\n",
        "dict_order = {\"Вид изделия\": \"Одежда\",\n",
        "                      \"Тип одежды\": \"Трикотажная одежда, Домашняя одежда\",\n",
        "                      #\"Название\": \"Костюм трикотажный\",\n",
        "                      \"Назначение\": \"Повседневная одежда\",\n",
        "                      \"По полу и возрасту\": \"Женская одежда\",\n",
        "                      \"Сезон\": \"Всесезон\",\n",
        "                      \"Ценовой сегмент\": \"Средний, Средний Плюс\",\n",
        "                      \"Заказчик предоставит\": \"Лекала, Фото изделия\",\n",
        "                      \"Конструирование\": \"Требуется разработка лекал\",\n",
        "                      \"Виды нанесения\": \"Бирка, Штамп\",\n",
        "                      \"Требования к оборудованию\": \"Пуговичная машина с приспособлением для пуговиц на ножке\",\n",
        "                      \"Комментарий к заказу\": \"Просим изготовить домашние костюмы для женщин от 50 лет и старше. Размерный ряд от 48 до 60\",\n",
        "                      \"Регионы производства\": \"ЦФО\", \"Образец\":\"Да\", \"Пробная партия\":\"50 единиц\",\n",
        "                      \"Плановый бюджет\":\"350000\",\n",
        "                      \"Наличие в реестре Минпромторга\": \"Да\",\n",
        "                      \"Требование к штату\":\"Не менее 8 человек в бригаде. Технолог швейного производства в штате\",\n",
        "                      \"Упаковка\":\"Лейблы и бирки\",\n",
        "                      \"Условия оплаты\":\"Аванс 50%\",\n",
        "                      \"Дополнительные услуги\": \"ВТО, упаковка\",\n",
        "                      \"Сертификация\":\"Обычная одежда, не требует обязательной сертификации\",\n",
        "                      \"Обеспечение сырьем\":\"Закупает фабрика\"\n",
        "                    }"
      ],
      "metadata": {
        "id": "i0KBZWlJWYKN"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "tables=files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "cTpu7iDBzzqC",
        "outputId": "d4fa2b50-7cf3-49e1-fe34-d723f7a73d44"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-09558e63-c04c-46eb-a887-b65bd9dcbf11\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-09558e63-c04c-46eb-a887-b65bd9dcbf11\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving ТЗ для Датасетов доп фабрики.xlsx to ТЗ для Датасетов доп фабрики.xlsx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xl = pd.ExcelFile('ТЗ для Датасетов доп фабрики.xlsx')\n",
        "df_fabrics = xl.parse('Фабрики')\n",
        "\n",
        "\n",
        "#\t'По сезону',"
      ],
      "metadata": {
        "id": "g26Org-t7ZXN"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "          #Подготовка данных по фабрикам\n",
        "df_fabrics_min=df_fabrics[['ID пользователя','Вид изделий',\t'Тип одежды',\t'Сфера применения',\t'По полу/возрасту',\t'Ценовые сегменты',\t'Регион производства',\n",
        "                                                 'Конструирование','Сырье',\t'Нанесение',\t'Оборудование',\t'Штат',\t'Упаковка']].copy()\n"
      ],
      "metadata": {
        "id": "Mry1ex6tB-Q_"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_fabrics_min = df_fabrics_min.astype(str)"
      ],
      "metadata": {
        "id": "NSYVSXKXNvfZ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_fabrics_min['fabrics_all_info'] = df_fabrics_min[['Вид изделий',\t'Тип одежды',\t'Сфера применения',\t'По полу/возрасту',\t'Ценовые сегменты',\t'Регион производства',\n",
        "                                                 'Конструирование','Сырье',\t'Нанесение',\t'Оборудование',\t'Штат',\t'Упаковка']].agg(' '. join , axis= 1 )"
      ],
      "metadata": {
        "id": "r4Nl7Fq6N99D"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df_fabrics_min.drop(['Вид изделий',\t'Тип одежды',\t'Сфера применения',\t'По полу/возрасту',\t'Ценовые сегменты',\t'Регион производства',\n",
        "                                                 'Конструирование','Сырье',\t'Нанесение',\t'Оборудование',\t'Штат',\t'Упаковка'], axis=1, inplace= True)"
      ],
      "metadata": {
        "id": "EZvtZP-QPkgk"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fabrics_info=df_fabrics_min['fabrics_all_info'].str.replace('nan', '')  #удалим nan из описания\n",
        "fabrics_info=df_fabrics_min['fabrics_all_info'].str.replace(',', '')\n",
        "fabrics_info=df_fabrics_min['fabrics_all_info'].str.lower()"
      ],
      "metadata": {
        "id": "MtNodR_5P5xB"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "list_values_for_new_dict = set(('Вид изделия','Название','Тип одежды','Назначение','Комментарий к заказу'))\n",
        "order_for_model = {k:v for k,v in dict_order.items() if k in list_values_for_new_dict} #заказ в модель\n",
        "fix_features = [k for k,v in dict_order.items() if k not in list_values_for_new_dict]  #неиспользуемые в модели поля для заказа\n",
        "\n",
        "order_min = pd.DataFrame(list(order_for_model.values ()))\n",
        "order_min=order_min.transpose()  #транспонируем\n",
        "order_min.columns = list(order_for_model.keys())  #назовем столбцы по названию полей заказа\n",
        "\n",
        "            #объединим данные в строку\n",
        "order_min['orders_all_info'] = order_min. agg(' '. join , axis= 1 )\n",
        "\n",
        "            #преобразуем - уберем пустые значения, приведем к нижнему регистру\n",
        "order_min['orders_all_info']=order_min['orders_all_info'].str.replace('nan', '')  #удалим nan из описания\n",
        "order_min['orders_all_info']=order_min['orders_all_info'].str.replace(',', '')\n",
        "order_min['orders_all_info']=order_min['orders_all_info'].str.lower()\n",
        "\n",
        "            #выделим описания, по которым будем сравнивать заказы\n",
        "text_to_compare = order_min['orders_all_info']\n",
        "# импортируем класс для лемматизации\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# создаём объект этого класса\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# и пустой список для слов после лемматизации\n",
        "lemmatized = []\n",
        "\n",
        "# проходимся по всем токенам\n",
        "for token in text_to_compare:\n",
        "\n",
        "    # применяем лемматизацию\n",
        "    token_order = lemmatizer.lemmatize(token)\n",
        "\n",
        "    # добавляем слово после лемматизации в список\n",
        "    lemmatized.append(token)\n",
        "\n",
        "#выделим основы слов в описании заказа\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "\n",
        "stemmer = SnowballStemmer(\"russian\")\n",
        "orders_stemmed=[]  #сюда сформируем новый перечень описаний фабрик - только основы слов\n",
        "for str in lemmatized:\n",
        " stemmed_words_orders = [stemmer.stem(word) for word in str.split()]\n",
        " row_stemmed_order = ' '.join(stemmed_words_orders)\n",
        " orders_stemmed.append(row_stemmed_order)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "          #'''Функция предобработки данных из файла фабрик.'''\n",
        "\n",
        "#xl = pd.ExcelFile('ТЗ для Датасетов доп фабрики.xlsx')\n",
        "#df_fabrics = xl.parse('Фабрики')\n",
        "\n",
        "\n",
        "          #Подготовка данных по фабрикам\n",
        "\n",
        "          #Подготовка данных по фабрикам\n",
        "#df_fabrics_min=df_fabrics[['Вид изделий',\t'Тип одежды',\t'Сфера применения',\t'По полу/возрасту',\t'Ценовые сегменты',\t'Регион производства',\n",
        "                                                # 'Конструирование','Сырье',\t'Нанесение',\t'Оборудование',\t'Штат',\t'Упаковка']].copy()\n",
        "\n",
        "\n",
        "#df_fabrics_min = df_fabrics_min.astype(str)\n",
        "\n",
        "#df_fabrics_min['fabrics_all_info'] = df_fabrics_min.agg(' '. join , axis= 1 )\n",
        "\n",
        "#df_fabrics_min(['Регистрация',\t'Логотип',\t'Название',\t'Email',\t'Телефон',\t'Сайт',\t'Имя контактного лица', 'Тип работ', 'Технология','Пошив образцов',\n",
        "                     #'Минимальная партия, шт.','Минимальная сумма заказа, руб','Условия оплаты',\t'Заказчик должен предоставить',\n",
        "                     #'Дополнительные услуги',\t'Сертификация',\t'Загруженность','Наличии в реестре Минпромторга'], axis= 1 , inplace= True)\n",
        "\n",
        "\n",
        "#df_fabrics_min.drop(['Вид изделий',\t'Тип одежды',\t'Сфера применения',\t'По полу/возрасту',\t'Ценовые сегменты',\t'Регион производства',\n",
        "                                                 #'Конструирование','Сырье',\t'Нанесение',\t'Оборудование',\t'Штат',\t'Упаковка'], axis=1, inplace= True)\n",
        "\n",
        "\n",
        "           #преобразуем - уберем пустые значения, приведем к нижнему регистру\n",
        "\n",
        "\n",
        "#fabrics_info=df_fabrics_min['fabrics_all_info'].str.replace('nan', '')  #удалим nan из описания\n",
        "#fabrics_info=df_fabrics_min['fabrics_all_info'].str.replace(',', '')\n",
        "#fabrics_info=df_fabrics_min['fabrics_all_info'].str.lower()\n",
        "\n",
        "\n",
        "       #Лемматизация импортируем класс для лемматизации\n",
        "\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "           # создаём объект этого класса\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "          # и пустой список для слов после лемматизации\n",
        "lemmatized = []\n",
        "\n",
        "          # проходимся по всем токенам\n",
        "for token in df_fabrics_min['fabrics_all_info']:\n",
        "\n",
        "          # применяем лемматизацию\n",
        "  token = lemmatizer.lemmatize(token)\n",
        "\n",
        "          # добавляем слово после лемматизации в список\n",
        "  lemmatized.append(token)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "         ##Стеммизация - выделим основы слов в описании фабрик\n",
        "import nltk\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "\n",
        "stemmer = SnowballStemmer(\"russian\")\n",
        "fabrics_stemmed=[]  #сюда сформируем новый перечень описаний фабрик - только основы слов\n",
        "for str in lemmatized:\n",
        "  stemmed_words = [stemmer.stem(word) for word in str.split()]\n",
        "  row_stemmed = ' '.join(stemmed_words)\n",
        "  fabrics_stemmed.append(row_stemmed)\n",
        "         #добавим сокращенные описания в датафрейм\n",
        "df_fabrics_min[\"reduced_desript\"]=fabrics_stemmed\n",
        "\n",
        "\n",
        "       #токенизируем все сокращенные тексты в нашем датасете\n",
        "def tokenize_in_df(strin):\n",
        "          try:\n",
        "             return list(tokenize(strin, deacc=True,))\n",
        "          except:\n",
        "             return \"\"\n",
        "df_fabrics_min[\"tokens\"] = df_fabrics_min[\"reduced_desript\"].apply(tokenize_in_df)\n",
        "\n",
        "      #Создадим словарь слов, которые есть во всем нашем наборе текстов:\n",
        "dictionary = corpora.Dictionary(df_fabrics_min[\"tokens\"])\n",
        "feature_cnt = len(dictionary.token2id)\n",
        "      #dictionary.token2id\n",
        "\n",
        "      #Создаем корпус, превратив наши токенизированные тексты в векторы\n",
        "corpus = [dictionary.doc2bow(text) for text in df_fabrics_min[\"tokens\"]]\n",
        "\n",
        "      # модель tf-idf\n",
        "tfidf = models.TfidfModel(corpus)\n",
        "index = similarities.SparseMatrixSimilarity(tfidf[corpus],num_features = feature_cnt)\n",
        "      #векторы наших опорных текстов и получив значения их похожестей из матрицы.\n",
        "for text in orders_stemmed:\n",
        "  kw_vector = dictionary.doc2bow(tokenize(text))\n",
        "  df_fabrics_min[text] = index[tfidf[kw_vector]]\n",
        "\n",
        "\n",
        "      #можем избавиться от заведомо непохожих текстов, посчитав сумму весов и оставив тексты с самыми высокими суммами\n",
        "df_fabrics_min[\"sum\"] = 0\n",
        "for text in orders_stemmed:\n",
        "   df_fabrics_min[\"sum\"] = df_fabrics_min[\"sum\"]+df_fabrics_min[text]\n",
        "\n",
        "      #Избавляться от лишних текстов можно обрезав по порогу суммы, или отсортировав по сумме и обрезав датасет по количеству текстов.\n",
        "df_fabrics_min[\"sum\"].value_counts(bins=5)\n",
        "\n",
        "df_fabrics_results=df_fabrics_min[df_fabrics_min[\"sum\"]>0.02]\n",
        "df_fabrics_results = df_fabrics_results.sort_values(by='sum', ascending=False)\n",
        "id_fabrics=df_fabrics_results['ID пользователя'].to_list()\n",
        "\n",
        "dict_for_code={'companies':id_fabrics, 'fix_features': fix_features}\n",
        "\n"
      ],
      "metadata": {
        "id": "j4d_NBPAWYUB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "522e7a14-d1d0-4e7f-8557-024443bc9a25"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    }
  ]
}